{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOkYA96432IsaHuYX0xwaV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark import SparkConf, SparkContext\n","import math\n","import string\n","import nltk\n","import csv\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","from nltk.corpus import stopwords\n","from collections import defaultdict, Counter\n","\n","\n","if 'sc' not in locals():\n","    conf = SparkConf().setMaster(\"local\").setAppName(\"Inverted Index & TF-IDF\")\n","    sc = SparkContext.getOrCreate(conf=conf)\n","    spark = SparkSession(sc)\n","\n","nltk.download('stopwords', quiet=True)\n","stop_words = set(stopwords.words('italian'))\n","\n","\n","def preprocess_description(description):\n","    remove_keywords = {\n","        \"affordable\", \"business\", \"office\",\n","        \"wlan\", \"devices\", \"dvd\", \"wifi\", \"installato\", \"365\", \"mouse\", \"tastiera\",\n","        \"supporto\", \"bluetooth\", \"espansione\", \"ricondizionato\", \"professional\",\n","        \"smartphone\", \"rpm\", \"•\", \"|\", \"【\", \"】\"\n","    }\n","    description = description.lower()\n","    description = description.replace('/', ' ').replace('丨', ' ')\n","    description = re.sub(r'([^\\w\\s’])', r' \\1 ', description)\n","    description = description.translate(str.maketrans('', '', string.punctuation))\n","    tokens = description.split()\n","    processed_tokens = [word for word in tokens if word not in stop_words and word not in remove_keywords]\n","    return ' '.join(processed_tokens)\n","\n","\n","def load_data(file_path):\n","    df = spark.read.option(\"delimiter\", \"\\t\").csv(file_path, header=True, inferSchema=True, encoding=\"UTF-8\")\n","    df = df.dropDuplicates(['Description'])\n","    products_rdd = df.rdd.map(lambda row: (row['Description'], row['Price'], row['URL'], preprocess_description(row['Description'])))\n","    return products_rdd\n","\n","\n","def build_inverted_index(products_rdd):\n","    document_frequencies = Counter()\n","    inverted_index = products_rdd \\\n","        .zipWithIndex() \\\n","        .flatMap(lambda x: [((term, x[1]), 1) for term in x[0][3].split()]) \\\n","        .reduceByKey(lambda a, b: a + b) \\\n","        .map(lambda x: (x[0][0], (x[0][1], x[1]))) \\\n","        .groupByKey() \\\n","        .mapValues(list) \\\n","        .collectAsMap()\n","\n","    for term, postings in inverted_index.items():\n","        document_frequencies[term] = len(postings)\n","\n","    num_documents = products_rdd.count()\n","    idf = {term: math.log(num_documents / (1 + df)) for term, df in document_frequencies.items()}\n","    return inverted_index, idf\n","\n","\n","def compute_tfidf_vectors(products_rdd, idf):\n","    def calculate_tfidf(doc_id, terms):\n","        term_counts = Counter(terms.split())\n","        return {term: (count / len(term_counts)) * idf[term] for term, count in term_counts.items() if term in idf}\n","\n","    tfidf_vectors = products_rdd \\\n","        .zipWithIndex() \\\n","        .map(lambda x: (x[1], calculate_tfidf(x[1], x[0][3]))) \\\n","        .collectAsMap()\n","\n","    return tfidf_vectors\n","\n","\n","def query_products(query, tfidf_vectors, idf):\n","    query = preprocess_description(query)\n","    query_terms = query.split()\n","    query_vector = {term: (query_terms.count(term) / len(query_terms)) * idf.get(term, 0) for term in query_terms}\n","\n","    query_array = np.array([query_vector.get(term, 0) for term in idf.keys()])\n","    similarities = []\n","\n","    for doc_id, doc_vector in tfidf_vectors.items():\n","        doc_array = np.array([doc_vector.get(term, 0) for term in idf.keys()])\n","        similarity = cosine_similarity([query_array], [doc_array])[0][0]\n","        similarities.append((doc_id, similarity))\n","\n","    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n","    return similarities[:5]\n","\n","# Funzione per salvare i risultati della query\n","def save_query_results(results, products_rdd, filename=\"/content/sample_data/dataset/query_results.tsv\"):\n","    products_list = products_rdd.collect()\n","    rows = []\n","    for doc_id, score in results:\n","        product = products_list[doc_id]\n","        rows.append([product[0], product[1], product[2], score])\n","\n","    df = pd.DataFrame(rows, columns=[\"Original Description\", \"Price\", \"URL\", \"Cosine Similarity\"])\n","    df.to_csv(filename, index=False, sep='\\t', quoting=3)\n","\n","# Funzione per salvare l'inverted index\n","def save_inverted_index(inverted_index, filename=\"/content/sample_data/dataset/inverted_index.tsv\"):\n","    with open(filename, 'w', newline='', encoding='utf-8') as file:\n","        writer = csv.writer(file, delimiter='\\t', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n","        writer.writerow([\"Termine\", \"Document ID\", \"Term Frequency\"])\n","        for term, postings in inverted_index.items():\n","            for doc_id, freq in postings:\n","                writer.writerow([term, doc_id, freq])\n","\n","\n","if __name__ == \"__main__\":\n","\n","    file_path = '/content/sample_data/dataset/products.tsv'\n","    products_rdd = load_data(file_path)\n","\n","    inverted_index, idf = build_inverted_index(products_rdd)\n","    save_inverted_index(inverted_index)\n","    print(\"Inverted Index is saved in inverted_index.tsv\")\n","\n","    tfidf_vectors = compute_tfidf_vectors(products_rdd, idf)\n","    query = input(\"Inserisci i termini della query: \")\n","    results = query_products(query, tfidf_vectors, idf)\n","    save_query_results(results, products_rdd)\n","    print(\"The results of the query are saved in query_results.tsv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61SFul5kFQY1","executionInfo":{"status":"ok","timestamp":1731871435394,"user_tz":-60,"elapsed":5377,"user":{"displayName":"davide mazzatenta","userId":"00096403887896249963"}},"outputId":"b91db962-dd7d-4975-8801-dd55f4cc076d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Inverted Index is saved in inverted_index.tsv\n","Inserisci i termini della query: HP Notebook 250\n","The results of the query are saved in query_results.tsv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lQWHFnxdlBOw"},"execution_count":null,"outputs":[]}]}